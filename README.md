# Project Structure Overview

This project is a lightweight, modular, local-first file assistant application that monitors, sorts, and answers queries about files using RAG (Retrieval-Augmented Generation) and LLMs (Local Language Models). It is optimized for low resource usage, efficient file handling, and seamless GUI integration (though GUI logic is kept separate).

## ğŸ“ Project Directory Structure

```
project_root/
â”‚
â”œâ”€â”€ main.py
â”‚   - Optional entry point or CLI launcher.
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ folders.json            # List of folder categories for classification.
â”‚   â””â”€â”€ settings.yaml           # Paths, model configs, LLM options, indexing rules.
â”‚
â”œâ”€â”€ core/                       # âœ… Centralized logic for shared tasks
â”‚   â”œâ”€â”€ extractor.py            # Extract text from PDFs, DOCX, and TXT files.
â”‚   â”œâ”€â”€ embedder.py             # Embeds text using sentence-transformers.
â”‚   â”œâ”€â”€ retriever.py            # Wraps FAISS search functionality.
â”‚   â”œâ”€â”€ file_ops.py             # Handles file/folder moves, renames, validation.
â”‚   â”œâ”€â”€ logger.py               # Logs classification steps and user feedback.
â”‚   â”œâ”€â”€ feedback.py             # Feedback schema and manager for review/correction.
â”‚   â””â”€â”€ pipeline_utils.py       # Reusable orchestration logic for sorting and indexing.
â”‚
â”œâ”€â”€ kb_builder.py               # Index all files from organized folders to build FAISS KB.
â”œâ”€â”€ file_watcher.py             # Monitors incoming raw files and triggers sorting.
â”œâ”€â”€ sorter_pipeline.py          # Extract â†’ Classify â†’ Move â†’ Log pipeline using LLM.
â”œâ”€â”€ query_engine.py             # RAG-based question answering over local files.
â”œâ”€â”€ llm_interface.py            # Sends prompts to and receives responses from local LLM.
â”‚
â”œâ”€â”€ index/
â”‚   â”œâ”€â”€ faiss_index.bin         # FAISS vector store for embedded files.
â”‚   â””â”€â”€ docs_metadata.json      # Maps vector entries to file paths and summaries.
â”‚
â”œâ”€â”€ feedback/
â”‚   â”œâ”€â”€ sorting_feedback.jsonl  # One-line-per-file classification logs.
â”‚   â””â”€â”€ corrections/            # User-corrected file entries (for future tuning).
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_files/              # Folder watched for incoming unorganized files.
â”‚   â””â”€â”€ organized/              # Final destination folders for sorted content.
â”‚
â””â”€â”€ models/
    â””â”€â”€ local_llm/              # Local LLM binaries (GGUF for llama.cpp or Ollama models).
```

## ğŸ§  Features
- **RAG-based File Sorting**: Classifies files into categories using extracted content + LLM.
- **Semantic Query Answering**: Lets users query local files using natural language.
- **Feedback Mechanism**: Every sorting decision is logged for user review/correction.
- **Modular Architecture**: Core logic is decoupled from GUI, allowing CLI, service, or frontend integration.
- **Local-first Execution**: No cloud API calls; everything is optimized for offline use.